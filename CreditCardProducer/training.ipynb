{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Generate customer data\n",
    "customers = []\n",
    "for _ in range(100):\n",
    "    customer = {\n",
    "        'cc_num': fake.credit_card_number(card_type=None),\n",
    "        'first': fake.first_name(),\n",
    "        'last': fake.last_name(),\n",
    "        'gender': fake.random_element(elements=('M', 'F')),\n",
    "        'street': fake.street_address(),\n",
    "        'city': fake.city(),\n",
    "        'state': fake.state(),\n",
    "        'zip': fake.zipcode(),\n",
    "        'lat': fake.latitude(),\n",
    "        'long': fake.longitude(),\n",
    "        'job': fake.job(),\n",
    "        'dob': fake.date_of_birth(minimum_age=18, maximum_age=90).strftime('%Y-%m-%d')\n",
    "    }\n",
    "    customers.append(customer)\n",
    "\n",
    "df_customers = pd.DataFrame(customers)\n",
    "df_customers.to_csv('customer.csv', index=False)\n",
    "\n",
    "# Display first few rows to ensure customers are generated correctly\n",
    "print(df_customers.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Load customer data\n",
    "df_customers = pd.read_csv('customer.csv')\n",
    "\n",
    "transaction_data = []\n",
    "\n",
    "# Define some categories for transactions\n",
    "categories = ['food', 'entertainment', 'groceries', 'gas_transport', 'clothing', 'health', 'misc_net', 'misc_pos']\n",
    "\n",
    "# Generate transaction data based on customers\n",
    "for _ in range(10000):\n",
    "    customer = df_customers.sample(1).iloc[0]\n",
    "    transaction_date = fake.date_time_this_year()\n",
    "    transaction = {\n",
    "        'cc_num': customer['cc_num'],\n",
    "        'first': customer['first'],\n",
    "        'last': customer['last'],\n",
    "        'trans_num': fake.uuid4(),\n",
    "        'trans_date': transaction_date.strftime('%Y-%m-%d'),\n",
    "        'trans_time': transaction_date.strftime('%H:%M:%S'),\n",
    "        'unix_time': int(time.mktime(transaction_date.timetuple())),\n",
    "        'category': random.choice(categories),\n",
    "        'amt': round(random.uniform(1, 1000), 2),\n",
    "        'merchant': fake.company(),\n",
    "        'merch_lat': fake.latitude(),\n",
    "        'merch_long': fake.longitude(),\n",
    "        'is_fraud': random.choice([0, 1])\n",
    "    }\n",
    "    transaction_data.append(transaction)\n",
    "\n",
    "df_transactions = pd.DataFrame(transaction_data)\n",
    "df_transactions.to_csv('transaction_training.csv', index=False)\n",
    "\n",
    "# Display first few rows to ensure transactions are generated correctly\n",
    "print(df_transactions.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from math import sqrt\n",
    "import datetime\n",
    "\n",
    "# Your MongoDB Atlas connection string with the database name\n",
    "atlas_connection_string = \"mongodb+srv://mungunoble:ZXeFVujGHAJszjWH@cluster0.8zkuwy4.mongodb.net/rtfs?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FraudDetection\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", f\"{atlas_connection_string}&collection=customers\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", f\"{atlas_connection_string}&collection=customers\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "csv_file_path = \"customer.csv\"\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Select the first 10 customers\n",
    "first_10_customers = df.limit(10)\n",
    "\n",
    "# Write the first 10 customers to MongoDB\n",
    "first_10_customers.write.format(\"mongo\").mode(\"append\").option(\"database\", \"rtfs\").option(\"collection\", \"customers\").save()\n",
    "\n",
    "# Print the message\n",
    "print(\"First 10 customers posted into DB\")\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "\n",
    "\n",
    "# # Define UDFs to calculate age and distance\n",
    "# def calculate_age(dob):\n",
    "#     return datetime.datetime.now().year - datetime.datetime.strptime(dob, '%Y-%m-%d').year\n",
    "\n",
    "# def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "#     return sqrt((lat1 - lat2)**2 + (lon1 - lon2)**2)\n",
    "\n",
    "# age_udf = udf(calculate_age, IntegerType())\n",
    "# distance_udf = udf(calculate_distance, DoubleType())\n",
    "\n",
    "# # Read the CSV files into Spark DataFrames\n",
    "# df_customers = spark.read.csv(\"customer.csv\", header=True, inferSchema=True)\n",
    "# df_transactions = spark.read.csv(\"transaction_training.csv\", header=True, inferSchema=True)\n",
    "# # Calculate age\n",
    "# df_customers = df_customers.withColumn('age', age_udf(col('dob')))\n",
    "\n",
    "# # Join transaction data with customer data to get customer latitude and longitude\n",
    "# data = df_transactions.join(df_customers, df_transactions.cc_num == df_customers.cc_num)\n",
    "\n",
    "# # Calculate distance using customer and merchant locations\n",
    "# data = data.withColumn('distance', distance_udf(\n",
    "#     col('lat'), col('long'), col('merch_lat'), col('merch_long')\n",
    "# ))\n",
    "\n",
    "# # Write data to MongoDB\n",
    "# try:\n",
    "#     data.write.format(\"mongo\").mode(\"append\").save()\n",
    "#     print(\"Data has been posted in MongoDB\")\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, year, lit\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "# MongoDB Atlas connection string\n",
    "atlas_connection_string = \"mongodb+srv://mungunoble:ZXeFVujGHAJszjWH@cluster0.8zkuwy4.mongodb.net/rtfs?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "\n",
    "# UDF to calculate age based on birth year\n",
    "def calculate_age(birth_year):\n",
    "    today = datetime.date.today()\n",
    "    current_year = today.year\n",
    "    return current_year - birth_year\n",
    "\n",
    "calculate_age_udf = udf(calculate_age, IntegerType())\n",
    "\n",
    "# UDF to calculate distance using the Haversine formula\n",
    "def calculate_distance(customer_lat, customer_lon, merchant_lat, merchant_lon):\n",
    "    earth_radius = 6371  # Kilometers\n",
    "\n",
    "    # Convert to radians\n",
    "    customer_lat_rad = customer_lat * math.pi / 180\n",
    "    customer_lon_rad = customer_lon * math.pi / 180\n",
    "    merchant_lat_rad = merchant_lat * math.pi / 180\n",
    "    merchant_lon_rad = merchant_lon * math.pi / 180\n",
    "\n",
    "    # Calculate distance formula\n",
    "    dlon = merchant_lon_rad - customer_lon_rad\n",
    "    dlat = merchant_lat_rad - customer_lat_rad\n",
    "    a = math.sin(dlat / 2) ** 2 + math.cos(customer_lat_rad) * math.cos(merchant_lat_rad) * math.sin(dlon / 2) ** 2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = earth_radius * c\n",
    "\n",
    "    return distance\n",
    "\n",
    "calculate_distance_udf = udf(calculate_distance, DoubleType())\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FraudDetection\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read customer data (replace with your actual path and format)\n",
    "customer_df = spark.read.csv(\"customer.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Read transaction data (replace with your actual path and format)\n",
    "transaction_df = spark.read.csv(\"transaction_training.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Add 'age' feature based on date of birth\n",
    "# Assuming 'dob' is in 'yyyy-MM-dd' format and you need to extract the year\n",
    "customer_df = customer_df.withColumn(\"birth_year\", year(col(\"dob\")))\n",
    "\n",
    "# Calculate age using the birth year\n",
    "customer_df = customer_df.withColumn(\"age\", calculate_age_udf(col(\"birth_year\")))\n",
    "\n",
    "# Join customer and transaction data using cc_num (assuming it's unique)\n",
    "joined_df = transaction_df.join(customer_df, on=\"cc_num\", how=\"left\")\n",
    "\n",
    "# Add 'distance' feature using Euclidean distance (replace column names)\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"distance\",\n",
    "    calculate_distance_udf(\n",
    "        col(\"lat\"), col(\"long\"),  # Customer location columns\n",
    "        col(\"merch_lat\"), col(\"merch_long\")  # Merchant location columns\n",
    "    )\n",
    ")\n",
    "\n",
    "# Split data into fraud and non-fraud based on a \"is_fraud\" column (replace with your actual column)\n",
    "fraud_df = joined_df.where(col(\"is_fraud\") == lit(True))\n",
    "non_fraud_df = joined_df.where(col(\"is_fraud\") == lit(False))\n",
    "\n",
    "# Write fraud data to MongoDB\n",
    "fraud_df.write.format(\"mongo\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"uri\", atlas_connection_string) \\\n",
    "    .option(\"database\", \"rtfs\") \\\n",
    "    .option(\"collection\", \"fraud\") \\\n",
    "    .save()\n",
    "\n",
    "# Write non-fraud data to MongoDB\n",
    "non_fraud_df.write.format(\"mongo\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"uri\", atlas_connection_string) \\\n",
    "    .option(\"database\", \"rtfs\") \\\n",
    "    .option(\"collection\", \"non-fraud\") \\\n",
    "    .save()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute, col, rand\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# MongoDB Atlas connection string\n",
    "atlas_connection_string = \"mongodb+srv://mungunoble:ZXeFVujGHAJszjWH@cluster0.8zkuwy4.mongodb.net/rtfs?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "\n",
    "num_non_fraud_transactions = 1000 \n",
    "num_fraud_transactions = 500 \n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    ".appName(\"FraudDetection\") \\\n",
    ".config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "# Read fraud and non-fraud data from MongoDB\n",
    "fraud_df = spark.read.format(\"mongo\") \\\n",
    "    .option(\"uri\", atlas_connection_string) \\\n",
    "    .option(\"database\", \"rtfs\") \\\n",
    "    .option(\"collection\", \"fraud\") \\\n",
    "    .load()\n",
    "\n",
    "non_fraud_df = spark.read.format(\"mongo\") \\\n",
    "    .option(\"uri\", atlas_connection_string) \\\n",
    "    .option(\"database\", \"rtfs\") \\\n",
    "    .option(\"collection\", \"non-fraud\") \\\n",
    "    .load()\n",
    "\n",
    "# Combine fraud and non-fraud data\n",
    "combined_df = fraud_df.union(non_fraud_df)\n",
    "\n",
    "# Ensure the label column is of the correct type\n",
    "combined_df = combined_df.withColumn(\"is_fraud\", col(\"is_fraud\").cast(\"double\"))\n",
    "\n",
    "# Balancing the data\n",
    "fraud_oversampled_df = fraud_df.withColumn(\"weight\", rand()).orderBy(\"weight\").limit(num_non_fraud_transactions)\n",
    "\n",
    "# Undersampling the non-fraud transactions\n",
    "non_fraud_undersampled_df = non_fraud_df.withColumn(\"weight\", rand()).orderBy(\"weight\").limit(num_fraud_transactions)\n",
    "\n",
    "# Combine oversampled fraud and undersampled non-fraud transactions\n",
    "balanced_df = fraud_oversampled_df.union(non_fraud_undersampled_df)\n",
    "\n",
    "# Remove the temporary weight column\n",
    "balanced_df = balanced_df.drop(\"weight\")\n",
    "\n",
    "# Define columns for transformation\n",
    "categorical_cols = [\"category\", \"merchant\", \"gender\", \"job\"]\n",
    "date_cols = [\"trans_date\", \"dob\"]\n",
    "time_cols = [\"trans_time\"]\n",
    "numeric_cols = [\"amt\", \"lat\", \"long\", \"merch_lat\", \"merch_long\"]\n",
    "\n",
    "# Extract features from date and time columns\n",
    "for col_name in date_cols:   \n",
    "    balanced_df = balanced_df.withColumn(f\"{col_name}_year\", year(col(col_name)))\n",
    "    balanced_df = balanced_df.withColumn(f\"{col_name}_month\", month(col(col_name)))\n",
    "    balanced_df = balanced_df.withColumn(f\"{col_name}_day\", dayofmonth(col(col_name)))\n",
    "\n",
    "for col_name in time_cols:\n",
    "    balanced_df = balanced_df.withColumn(f\"{col_name}_hour\", hour(col(col_name)))\n",
    "    balanced_df = balanced_df.withColumn(f\"{col_name}_minute\", minute(col(col_name)))\n",
    "\n",
    "# Create StringIndexer and OneHotEncoder stages for categorical columns\n",
    "string_indexer_stages = [StringIndexer(inputCol=col, outputCol=col + \"_indexed\") for col in categorical_cols]\n",
    "onehot_encoder_stages = [OneHotEncoder(inputCol=col + \"_indexed\", outputCol=col + \"_encoded\") for col in categorical_cols]\n",
    "\n",
    "# Combine all feature columns\n",
    "feature_cols = numeric_cols + \\\n",
    "            [col + \"_year\" for col in date_cols] + \\\n",
    "            [col + \"_month\" for col in date_cols] + \\\n",
    "            [col + \"_day\" for col in date_cols] + \\\n",
    "            [col + \"_hour\" for col in time_cols] + \\\n",
    "            [col + \"_minute\" for col in time_cols] + \\\n",
    "            [col + \"_encoded\" for col in categorical_cols]\n",
    "\n",
    "# Create VectorAssembler to combine all encoded features\n",
    "vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Define the classifier\n",
    "classifier = RandomForestClassifier(labelCol=\"is_fraud\", featuresCol=\"features\")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=string_indexer_stages + onehot_encoder_stages + [vector_assembler, classifier])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "training_df, testing_df = balanced_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "# Train the model on the training data\n",
    "model = pipeline.fit(training_df)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "predictions = model.transform(testing_df)\n",
    "\n",
    "\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"is_fraud\", \"probability\").show(5)\n",
    "\n",
    "# Instantiate an evaluator for multiclass classification.\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"is_fraud\", rawPredictionCol=\"rawPrediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions)\n",
    "recall = evaluator.setMetricName(\"weightedRecall\").evaluate(predictions)\n",
    "f1 = evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Calculate AUC-ROC\n",
    "auc_roc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "\n",
    "# Train the model on the full dataset\n",
    "model = pipeline.fit(balanced_df)\n",
    "\n",
    "# Save the model\n",
    "model_path = \"Model\"\n",
    "model.write().overwrite().save(model_path)\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
